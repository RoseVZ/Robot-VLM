# Vision-Language-Guided Motion Planning for Instruction-Based Navigation

A robotics navigation system that combines Vision-Language Models (VLMs) with classical path planning algorithms to enable autonomous object search in unexplored indoor environments.

##  Table of Contents

- [Overview](#overview)
- [Features](#features)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Experimental Results](#experimental-results)
- [Troubleshooting](#troubleshooting)
- [Citation](#citation)
- [License](#license)

---

##  Overview

This project implements a **Progressive Confidence Navigation System** that enables a mobile robot to autonomously find specific objects (e.g., "find the fridge") in unexplored indoor apartments without prior maps or human intervention.

### Key Innovation

**Progressive Confidence Assessment**: Mimics human behavior by moving closer to verify uncertain object detections, combining:
- **GroundingDINO** for fine-grained object detection with spatial localization
- **GPT-4V** for semantic reasoning and room verification
- **A\*/Dijkstra** for efficient path planning
- **Frontier-based exploration** for systematic environment discovery

---

##  Features

### Multimodal Perception
-  **Open-vocabulary object detection** using GroundingDINO
-  **Semantic reasoning** with GPT-4V for room classification
-  **360Â° panoramic capture** (8 views at 45Â° intervals)
-  **Combined confidence scoring** (Î±=0.7 for detection, Î²=0.3 for semantics)

### Intelligent Navigation
-  **Two-stage verification framework**
  - Stage 1: Initial detection â†’ navigate (â‰¥0.80), approach (â‰¥0.60), or explore (<0.60)
  - Stage 2: Closer verification with stricter thresholds (â‰¥0.80)
-  **Real-time SLAM** with occupancy grid mapping (0.1m resolution)
- **Classical path planning** (A\*, Dijkstra's) with dynamic replanning
-  **Frontier-based exploration** using BFS wavefront detection

### Robust Design
- âœ… **Zero prior knowledge** required
- âœ… **False positive prevention** through progressive verification
- âœ… **Collision-aware navigation** with robot footprint (0.5m radius)
- âœ… **Fallback mechanisms** for API failures

---

##  System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Query                             â”‚
â”‚                  "Go to the fridge"                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Panoramic View Capture      â”‚
         â”‚   (8 views at 45Â° intervals)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GroundingDINO    â”‚    â”‚    GPT-4V       â”‚
â”‚ Object Detection â”‚    â”‚ Room Reasoning  â”‚
â”‚ (Spatial)        â”‚    â”‚ (Semantic)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Combined Confidence   â”‚
         â”‚ Î±Â·C_GD + Î²Â·C_VLM     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Progressive Decision â”‚
         â”‚   Stage 1 â†’ Stage 2   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Navigate    â”‚          â”‚   Explore    â”‚
â”‚  (A* Path)   â”‚          â”‚  (Frontier)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

##  Installation

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (optional, for faster inference)
- OpenAI API key or Anthropic API key

### Step 1: Clone Repository
```bash
git clone https://github.com/yourusername/vlm-navigation.git
cd vlm-navigation
```

### Step 2: Install Dependencies
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install required packages
pip install -r requirements.txt
```

**requirements.txt:**
```
numpy>=1.21.0
pybullet>=3.2.0
pillow>=9.0.0
opencv-python>=4.5.0
openai>=1.0.0
anthropic>=0.7.0
python-dotenv>=0.19.0
matplotlib>=3.5.0
torch>=2.0.0
transformers>=4.30.0
```

### Step 3: Download Kenny 3D Models
```bash
# Download Kenny's 3D models
mkdir object_models
cd object_models

# Download from: https://kenney.nl/assets/furniture-kit
# Extract .obj files to this directory

cd ..
```

### Step 4: Configure API Keys

Create a `.env` file in the project root:
```bash
# .env
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
```

**Important:** Add `.env` to `.gitignore` to prevent accidentally committing API keys!

---



## Configuration

### Environment Layouts

Three layouts available:

1. **Single Room** (8m Ã— 8m)
   - Simple kitchen with 5 objects
   - Baseline testing

2. **Multi-Room** (16m Ã— 16m)
   - Living room, bedroom, kitchen
   - 12-15 objects
   - Room transition testing

3. **Cluttered** (20m Ã— 20m)
   - 4 rooms with narrow passages
   - 20-25 objects
   - Stress testing


### Confidence Thresholds

Adjust in `vlm_room_verifier.py`:
```python
# Stage 1 thresholds
HIGH_CONFIDENCE = 0.80  # Navigate directly
MEDIUM_CONFIDENCE = 0.60  # Approach and verify

# Stage 2 threshold
VERIFICATION_THRESHOLD = 0.80  # After approaching

# Weighting
ALPHA = 0.7  # GroundingDINO weight
BETA = 0.3   # GPT-4V weight
```

---

## ðŸ“Š Experimental Results

### Detection Performance

| Method | Success Rate | False Positive Rate | Avg Confidence |
|--------|-------------|---------------------|----------------|
| **Combined (Ours)** | **92%** | **3%** | **0.85** |
| GroundingDINO only | 71% | 22% | 0.68 |
| GPT-4V only | 65% | 8% | 0.72 |
| Heuristic baseline | 45% | 35% | N/A |

### Path Planning Performance

**Single Room ("Go to fridge"):**

| Algorithm | Path Length | Planning Time | Success Rate |
|-----------|-------------|---------------|--------------|
| A* | 5.10m | 274.55ms | 100% |
| Dijkstra | 5.39m | 358.92ms | 100% |

**Multi-Room ("Go to table"):**

| Algorithm | Path Length | Planning Time | Success Rate |
|-----------|-------------|---------------|--------------|
| A* | 19.5m | 1032.88ms | 60% |
| Dijkstra | 21.3m | 1263.92ms | 50% |



---



##  Authors

- **Tanya Mehta** - [tm3517@columbia.edu](mailto:tm3517@columbia.edu)
- **Priyanka Rose Varghese** - [prv2108@columbia.edu](mailto:prv2108@columbia.edu)

---

## Acknowledgments

- [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) for open-vocabulary detection
- [OpenAI GPT-4V](https://openai.com/research/gpt-4v-system-card) for vision-language reasoning
- [Kenny](https://kenney.nl/) for 3D furniture models
- [PyBullet](https://pybullet.org/) for physics simulation


